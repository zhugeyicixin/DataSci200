{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression summary\n",
    "\n",
    "The logistic or sigmoid function can be written two equivalent ways:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)} = \\frac{\\exp(t)}{1 + \\exp(t)} $$\n",
    "\n",
    "The logistic regression model assumes the following probabilities of $Y \\in \\{0, 1\\}$ given column vector $X$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=1|X) &= \\sigma(X^T \\beta) &&= \\frac{1}{1 + \\exp(-X^T \\beta)} &= \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)} \\\\[10pt]\n",
    "P(Y=0|X) &= \\sigma(-X^T \\beta) &&= \\frac{1}{1 + \\exp(X^T \\beta)} &= \\frac{1}{1 + \\exp(X^T\\beta)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The loss most typically used to fit $\\beta$ is the log loss or cross-entropy loss, which is the negative log probability of the correct (observed) $Y$ value. This loss for true $Y \\in \\{0, 1\\}$ and predicted probability $\\hat Y \\in [0, 1]$ is often written:\n",
    "\n",
    "$$-Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    \"\"\"The logistic or sigmoid function, denoted σ(t).\n",
    "    \n",
    "    Note: This is actually a special case of what is generally \n",
    "          named the \"logistic\" function,\n",
    "          which allows for a different numerator and offset, \n",
    "          but lots of people call this the logistic function in practice.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def prediction(x, beta):\n",
    "    \"\"\"Prediction under the logistic model for features x and parameters b.\"\"\"\n",
    "    return sigma(x @ beta)\n",
    "\n",
    "def squared_loss(y, y_hat):\n",
    "    \"\"\"Squared loss applies to any true y and predicted y_hat.\"\"\"\n",
    "    return (y - y_hat) ** 2\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    \"\"\"Log loss or cross-entropy loss, assuming y is in [0, 1].\"\"\"\n",
    "    assert y in [0, 1]\n",
    "    return -y * np.log(y_hat) - (1-y) * np.log(1-y_hat)\n",
    "\n",
    "def empirical_risk(true_ys, predicted_ys, loss):\n",
    "    \"\"\"The empirical risk is the average loss for a sample.\"\"\"\n",
    "    losses = [loss(y, y_hat) for y, y_hat in zip(true_ys, predicted_ys)]\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk\n",
    "\n",
    "Filling in $\\hat Y = P(Y=1|X)$ and the form of the model, we find different ways of expressing the same loss:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta) &= -Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y) \\\\[10pt]\n",
    "         &= - Y \\log P(Y=1|X) - (1-Y) \\log P(Y=0|X)  \\\\[10pt]\n",
    "         &= - Y (\\log(\\exp(X^T\\beta)) - \\log(1 + \\exp(X^T\\beta))) - (1-Y) (-\\log (1 + \\exp(X^T\\beta)))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + Y \\log(1 + \\exp(X^T\\beta))) - Y \\log(1 + \\exp(X^T\\beta))) + \\log (1 + \\exp(X^T\\beta))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + \\log (1 + \\exp(X^T\\beta)) \\\\[10pt]\n",
    "         &= -\\left(YX^T\\beta + \\log \\sigma(-X^T\\beta)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where the last step follows from $\\log (1 + \\exp(X^T\\beta)) = -(- \\log (1 + \\exp(X^T\\beta))) = -\\log \\frac{1}{1 + \\exp(X^T\\beta)} = -\\log \\sigma(-X^T\\beta)$.\n",
    "\n",
    "The empirical risk (average loss for a sample) for a set of observations $(x_1, y_1) \\dots (x_n, y_n)$ is often written:\n",
    "\n",
    "$$R(\\beta, x, y) = - \\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient\n",
    "\n",
    "Using thelogistic regression model and log loss, find the gradient of the empirical risk.\n",
    "\n",
    "First, we compute the derivative of the sigmoid function since we'll use it in our gradient calculation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(t) &= \\frac{1}{1 + e^{-t}} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{e^{-t}}{(1 + e^{-t})^2} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{1}{1 + e^{-t}} \\cdot \\left(1 - \\frac{1}{1 + e^{-t}} \\right) \\\\[10pt]\n",
    "\\sigma'(t) &= \\sigma(t) (1 - \\sigma(t))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shorthand, we define $ \\sigma_i = \\sigma(-x_i^T \\beta) $. We will soon need the gradient of $ \\sigma_i $ with respect to the vector $ \\beta $ so we will derive it now using the chain rule. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\beta} \\sigma_i\n",
    "&= \\nabla_{\\beta} \\sigma(-x_i^T \\beta) \\\\[10pt]\n",
    "&= \\sigma\\left(-x_i^T \\beta\\right) \\left(1 - \\sigma(-x_i^T \\beta)\\right)  \\nabla_{\\beta} \\left(-x_i^T \\beta\\right) \\\\[10pt]\n",
    "&= -\\sigma_i (1 - \\sigma_i) x_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we derive the gradient of the cross-entropy loss with respect to the model parameters $ \\boldsymbol{\\beta} $. We use the fact that $(1-\\sigma_i) = \\sigma(x_i^T\\beta)$, since $\\sigma(x^T\\beta) + \\sigma(-x^T\\beta) = 1$.\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y) &= - \\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma_i \\right] \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y) &= - \\sum_{i=1}^n \\left( y_i x_i - \\frac{1}{\\sigma_i} \\sigma_i (1 - \\sigma_i) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\sum_{i=1}^n \\left( y_i x_i - \\sigma(x_i^T\\beta) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i  \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta0, gradient_function, max_iter=50000,  \n",
    "                     epsilon=1e-8, lr=0.1, clip=1):\n",
    "    \"\"\"Run gradient descent on a dataset (x, y) \n",
    "    with gradient clipping and learning rate decay.\"\"\"\n",
    "    beta = beta0\n",
    "    for t in range(1, max_iter):\n",
    "        grad = gradient_function(beta, x, y)\n",
    "        beta = beta - (lr/(1 + t/100)) * np.clip(grad, -clip, clip) \n",
    "        # Detect approximate convergence: small gradient\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            return beta\n",
    "    return beta\n",
    "\n",
    "def risk_gradient(beta, x, y):\n",
    "    \"\"\"Risk gradient for a whole dataset at once.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    return -(1/n) * x @ (y - sigma(x.T @ beta)) \n",
    "\n",
    "def logistic_regression(x, y):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, risk_gradient)\n",
    "    return beta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                  17.990000\n",
       "mean texture                 10.380000\n",
       "mean perimeter              122.800000\n",
       "mean area                  1001.000000\n",
       "mean smoothness               0.118400\n",
       "mean compactness              0.277600\n",
       "mean concavity                0.300100\n",
       "mean concave points           0.147100\n",
       "mean symmetry                 0.241900\n",
       "mean fractal dimension        0.078710\n",
       "radius error                  1.095000\n",
       "texture error                 0.905300\n",
       "perimeter error               8.589000\n",
       "area error                  153.400000\n",
       "smoothness error              0.006399\n",
       "compactness error             0.049040\n",
       "concavity error               0.053730\n",
       "concave points error          0.015870\n",
       "symmetry error                0.030030\n",
       "fractal dimension error       0.006193\n",
       "worst radius                 25.380000\n",
       "worst texture                17.330000\n",
       "worst perimeter             184.600000\n",
       "worst area                 2019.000000\n",
       "worst smoothness              0.162200\n",
       "worst compactness             0.665600\n",
       "worst concavity               0.711900\n",
       "worst concave points          0.265400\n",
       "worst symmetry                0.460100\n",
       "worst fractal dimension       0.118900\n",
       "bias                          1.000000\n",
       "malignant                     1.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "cancer['bias'] = 1.0\n",
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target']\n",
    "cancer.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>bias</th>\n",
       "      <th>malignant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension     ...      worst perimeter  \\\n",
       "count     569.000000              569.000000     ...           569.000000   \n",
       "mean        0.181162                0.062798     ...           107.261213   \n",
       "std         0.027414                0.007060     ...            33.602542   \n",
       "min         0.106000                0.049960     ...            50.410000   \n",
       "25%         0.161900                0.057700     ...            84.110000   \n",
       "50%         0.179200                0.061540     ...            97.660000   \n",
       "75%         0.195700                0.066120     ...           125.400000   \n",
       "max         0.304000                0.097440     ...           251.200000   \n",
       "\n",
       "        worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension   bias  \\\n",
       "count            569.000000      569.000000               569.000000  569.0   \n",
       "mean               0.114606        0.290076                 0.083946    1.0   \n",
       "std                0.065732        0.061867                 0.018061    0.0   \n",
       "min                0.000000        0.156500                 0.055040    1.0   \n",
       "25%                0.064930        0.250400                 0.071460    1.0   \n",
       "50%                0.099930        0.282200                 0.080040    1.0   \n",
       "75%                0.161400        0.317900                 0.092080    1.0   \n",
       "max                0.291000        0.663800                 0.207500    1.0   \n",
       "\n",
       "        malignant  \n",
       "count  569.000000  \n",
       "mean     0.372583  \n",
       "std      0.483918  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  426\n",
      "Test Data Size:  143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "radii = np.linspace(5, 30, 100)\n",
    "averages = [np.average(train[np.abs(train['mean radius']-r)<1]['malignant']) for r in radii]\n",
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(t):\n",
    "    return t[['bias', 'mean radius']].values.T\n",
    "    \n",
    "x_train, y_train = features(train), train['malignant'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14.8970826 ,   1.01064211]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n",
    "model.fit(x_train.T, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');\n",
    "plt.plot(radii, sigma(model.coef_[0,0] + radii * model.coef_[0,1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.86831235,   1.00862628])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = logistic_regression(x_train, y_train)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00645167, -0.00045204])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_gradient(beta, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/426 (86.9%)\n"
     ]
    }
   ],
   "source": [
    "def print_ratio(n, d):\n",
    "    print('{}/{} ({:.1f}%)'.format(n, d, 100 * n/d))\n",
    "\n",
    "def accuracy(x, y, beta):\n",
    "    y_hat = np.round(sigma(x.T @ beta))\n",
    "    guesses = np.round(y_hat)\n",
    "    correct = y == guesses\n",
    "    print_ratio(sum(correct), len(correct))\n",
    "\n",
    "accuracy(x_train, y_train, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/143 (60.8%)\n"
     ]
    }
   ],
   "source": [
    "x_test = features(test)\n",
    "y_test = test['malignant'].values\n",
    "print_ratio(sum(1-y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/143 (90.9%)\n"
     ]
    }
   ],
   "source": [
    "accuracy(x_test, y_test, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n"
     ]
    }
   ],
   "source": [
    "def all_features(t):\n",
    "    return t.drop('malignant', axis=1).values.T\n",
    "\n",
    "def evaluate(beta, features):\n",
    "    print('Train:', end=' ')\n",
    "    accuracy(features(train), y_train, beta)\n",
    "    print('Test:', end=' ')\n",
    "    accuracy(features(test), y_test, beta)\n",
    "    \n",
    "beta = logistic_regression(all_features(train), y_train)\n",
    "evaluate(beta, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "As with linear regression, one common way of reducing the variance of the parameter estimator is to add a regularization term to the empirical risk objective. E.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y, \\lambda) &= - \\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right] + \\frac{1}{2} C \\sum_{j=1}^J \\beta_j^2 \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y, \\lambda) &=  - \\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i + C \\beta \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  181.05862166469308\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  153.17656348299207\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  84.76727676186307\n",
      "Train: 401/426 (94.1%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  16.86808437919907\n",
      "Train: 396/426 (93.0%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  1.159763781457247\n",
      "Train: 392/426 (92.0%)\n",
      "Test: 135/143 (94.4%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  0.21983095016705734\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.08526863018405177\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.06240644243334323\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.043885815938907356\n",
      "Train: 388/426 (91.1%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.016109390288629684\n",
      "Train: 387/426 (90.8%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def regularized_logistic_regression(x, y, c):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "\n",
    "    def l2_regularized_gradient(beta, x, y):\n",
    "        return risk_gradient(beta, x, y) + c * beta\n",
    "\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, l2_regularized_gradient)\n",
    "    return beta    \n",
    "\n",
    "def search_for_c(features):\n",
    "    for c in 2.0 ** np.arange(-10, 10, 2):\n",
    "        print(\"c =\", c)\n",
    "        beta = regularized_logistic_regression(features(train), y_train, c)\n",
    "        print(\"sum(beta**2) = \", sum(beta**2))\n",
    "        evaluate(beta, features)\n",
    "        print()\n",
    "        \n",
    "search_for_c(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  35.8486017048037\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  31.318273815172542\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  20.747956964144077\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  9.935156093274612\n",
      "Train: 421/426 (98.8%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  4.279626098449614\n",
      "Train: 419/426 (98.4%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  1.717220435886093\n",
      "Train: 414/426 (97.2%)\n",
      "Test: 140/143 (97.9%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.6094833404635616\n",
      "Train: 412/426 (96.7%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.17553643009069883\n",
      "Train: 405/426 (95.1%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.03500234439319296\n",
      "Train: 403/426 (94.6%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.0041317425176505785\n",
      "Train: 399/426 (93.7%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def inputs(t):\n",
    "    return t.drop('malignant', axis=1).values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(inputs(train))\n",
    "\n",
    "def scaled_features(t):\n",
    "    return scaler.transform(inputs(t)).T\n",
    "\n",
    "search_for_c(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/143 (96.5%)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=4, solver='lbfgs')\n",
    "model.fit(scaled_features(train).T, y_train)\n",
    "y_hat = model.predict(scaled_features(test).T)\n",
    "print_ratio(sum(y_hat == y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=y|X) &= \\frac{\\exp(X^T\\beta_{y})}{\\sum_{z=0}^d \\exp(X^T\\beta_z)} \\\\[10pt]\n",
    "L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= - \\log \\frac{\\exp(x_i^T\\beta_{y_i})}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)} \\\\[10pt]\n",
    "\\frac{\\partial}{\\partial \\beta_w} L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= -\\left(1[w=y_i] - \\frac{\\exp(x_i^T\\beta_w)}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)}\\right) x_i  \\\\[10pt]\n",
    "1[w=y_i] &= \\begin{cases}\n",
    "1 & \\text{if}\\ w=y_i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = sklearn.datasets.load_iris()\n",
    "x = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "y = data_dict['target']\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([50, 50, 50]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/75 (96.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=100)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "y_hat = model.predict(x_test)\n",
    "print_ratio(sum(y_hat == y_test), len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
